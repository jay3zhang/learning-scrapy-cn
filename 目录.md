# 目录

关于本书

幕后人员

关于作者

关于审阅者

关于出版方www.PacktPub.com
- 支持文件、 电子书、 折扣优惠和更多
     + 为什么应该订阅？
     + Packt 帐户持有人免费拥有


## 前言
- 本书内容
- 你需要准备什么
- 本书适用的人群
- 一些约定
- 读者反馈
- 客户支持
     + 下载代码示例
     + 勘误表
     + 关于盗版
     + 问题反馈
        

## 1. 介绍scrapy
 	* 你好，scrapy
 	* 更多喜欢scrapy的原因
 	* 关于本书：目的和用途
 	* 掌握自动化数据抓取的重要性
 		- 开发优质的应用程序，并提供切合实际的计划
 		- 快速开发微型好用的产品
 		- 爬取给与你scale；Google不使用表单
 		- Discovering and integrating into your ecosystem（不会翻）
 	* 在爬虫世界做一个好公民
 	* 关于scrapy的一些误解
 	* 总结
 
## 2. 理解HTML和XPath
	* HTML，DOM树形表示，and XPath
		- URL
		- HTML 文档
		- HTML的树形表示
		- 你在屏幕上看到的HTML
	* XPath选择器
		- XPath表达式
		- 使用Chrome获取XPath表达式
		- 常见示例
		- 预防网页变化
	* 总结

## 3. 爬虫基础
	* 安装scrapy
		- MacOS
		- windows
		- Linux
		- 获取最新的Scrapy
		- 更新Scrapy
		- 这本书运行示例的方式
	* 基本的爬取过程
		- url
		- reqtest and response
		- Items
	* 一个scrapy工程
		- 定义items
		- 写爬虫
		- 设置pipelines
		- 保存到文件
		- Cleaning up – item loaders and housekeeping fields 
		- Creating contracts
	* 提取更多的url
		- Two-direction crawling with a spider
		- Two-direction crawling with a CrawlSpider 
	* 总结

## 4. 利用scrapy构建一个移动应用
	* 选择一个移动应用框架
	* 创建一个数据库并连接
	* 通过scrapy操作数据库
	* 创建一个移动APP
		- 创建数据库访问服务
		- 设置用户界面
		- 将数据映射到用户界面
		- 数据库字段与用户界面控件之间的映射
		- 测试，共享和导出移动APP
	* 总结
	
## 5. 快速爬虫指南
	* 登录
	* 爬取使用JSON API和Ajax的页面
		- responses 之间传递参数
	* 一个快30倍的爬虫
	* 一个基于Excel文件的爬虫
	* 总结

## 6. 部署到Scrapinghub
	* 注册，登录，启动项目
	* 部署爬虫并运行
	* 访问我们的items
	* 安排周期性的运行
	* 总结

## 7. 配置和管理
	* 使用scrapy设置
	* 必要的设置
		- 分析
			+ logging 日志
			+ stats 统计
			+ telnet
		- 性能
		- 设置某些情况下爬虫提早停止
		- HTTP 缓存和脱机工作
		- 爬取的方式（例如：爬取深度）
		- feeds
		- 下载媒体文件（图片、视频等）
		- AWS的使用 
		- 使用代理
	* 更多设置
		- 工程相关的设置
		- 扩展scrapy设置
		- 对下载的微调
		- 自动调节流量的扩展设置
		- 内存使用设置
		- 日志和调试
	* 总结
	
## 8. scrapy编程
	* scrapy 是一个Twisted应用
		- 延迟
		- 理解 Twisted和非阻塞I/O
	* Scrapy的体系结构概述
		- 例1 -一个非常简单的pipeline
	* signals （一些事件的信号）
		- 例2 -一个测量吞吐量和延迟的扩展
	* 扩展中间件
	* 总结

## 9. pipeline指南
	* 使用 REST API
		- 使用 treq （一个python包）
		- 写入Elasticsearch的pipeline
		- 使用谷歌地图API
		- 在Elasticsearch中使用位置索引
	* 用标准python客户端与数据库交互
		- 写入MySQL的pipeline
	* 使用Twisted特殊的客户端与其他设备/服务交互
		- 一个对Redis进行读写的pipeline
	* 多CPU，阻塞，legacy functionality（不会翻）
		- 执行阻塞或多CPU操作
		- 使用二进制文件或脚本
	* 总结

## 10. 理解scrapy的性能
	* Scrapy引擎
		- 串联排队系统
		- 寻找性能瓶颈
		- scrapy的性能模型
	* 利用Telnet获取组件的利用率
	* 我们的参照系统
	* 标准性能模型
	* 解决性能问题
		- case 1：CPU饱和
		- case 2：代码阻塞
		- case 3：下载器里的无用数据
		- case 4：由于response太多或太大引起的溢出
		- case 5：由于item并发引起的溢出
		- case 6：downloader没有充分利用
	* 故障排除流程
	* 总结
	
## 11. 用scrapyd和实时分析构建分布式爬虫
	* 名字如何影响价格
	* scrapyd
	* 分布式系统概述
	* 改变爬虫和中间件
		- 分片索引爬取
		- 批量爬取url
		- 从设置获取初始url
		- 部署项目到scrapyd服务器
	* 创建自定义监测命令
	* 利用spark流计算shift（变化？）
	* 运行分布式爬虫
	* 系统性能
	* 这章的关键
	* 总结

## 附录