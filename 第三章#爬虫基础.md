# 爬虫基础

这是一个非常重要的章节，你可能会读几遍，并经常返回寻找解决方案。我们将首先解释如何安装Scrapy，然后继续随着众多实例和不同的实现方法开发scrapy爬虫。在开始之前，让我们先来看看一些重要的概念。

我们将互换地使用术语 'terminal(终端)'，'console(控制台)'，'command line(命令行)'，他们在本书中没有区别。

## 安装scrapy 
**具体的各操作系统的安装略，自行google或百度**
* Vagrant安装：本书运行示例的官方方法
![本书中用到的系统](./pic/3-1.png) 

你的电脑在Vagrant被称为主机（host），Vagrant 使用我们的主机运行一个虚拟机。这些技术使得我们有一个带私有网络的独立的环境，在运行示例时不受你主机软硬件的影响。

大多数章节只使用两个服务—"dev"和"web"。我们登录到"dev"运行scrapy 爬虫从"web"抓取页面。后面的章节使用更多的服务，包括数据库和大数据处理引擎。

按照附录A的指导在你的操作系统上安装 Vagrant。你可以这样做得到这本书的代码：
```
$ git clone https://github.com/scalingexcellence/scrapybook.git
$ cd scrapybook
```
你也可以用下面的命令开启 Vagrant ：
`$ vagrant up --no-parallel`
第一次运行可能会花一些时间，Vagrant开启后，登录虚拟的 dev 设备：
`$ vagrant ssh`
现在你在dev的控制台。代码已从主机复制到开发机器，您可以在"book" 文件夹中找到它：
```
$ cd book
$ ls
ch03 ch04 ch05 ch07 ch08 ch09 ch10 ch11…
```
打开几个控制台，用`vagrant ssh`打开多个开发终端。你可以使用 `vagrant halt` 关闭系统和 `vagrant status` 检查他们的状态。注意，`vagrant halt` 不会关闭虚拟机。如果这有问题，可以手动停止它，或者用`vagrant global-status`查找它的 id （名字为"docker-provider"），然后用`vagrant halt <ID>`关闭它。利用vagrant，离线状态下大多数的例子也都可以运行。

现在我们完成了系统的安装，准备开始学习scrapy。

## 基本的爬取过程
![](./pic/3-2.png)

* URL
一切开始于一个URL，你需要一些目标网站的URL。这将用 [Gumtree](https://www.gumtree.com/) 分类网站做示例演示。

例如，通过访问 http://www.gumtree.com/flats-houses/london 上 Gumtree 的伦敦房产页面，你将能够发现大量的房产URL。你可以复制他们通过在分类列表右击，然后点击 **Copy Link Address** 。这些URL看起来可能像 https://www.gumtree.com/p/studios-bedsits-rent/split-level 。这是没问题的对于真实网站的几个URL。但在Gumtree 网站改变之后，XPath表达式可能停止工作。另外，如果你不是用 user-agent，Gumtree将不会回复。你可以使用 scrapy shell 加载一个页面：
```
scrapy shell -s USER_AGENT="Mozilla/5.0" <your url here	e.g. http://www.gumtree.com/p/studios-bedsits-rent/...>
```
为了调试使用 scrapy shell 的问题，添加 `--pdb` 参数在异常情况下启用交互式调试。
`scrapy shell --pdb https://gumtree.com`

**Tip**
scrapy shell 是开发 scrapy 非常重要的工具。

显然，我们不鼓励你在学习本书时攻击 Guntree 的网站，而且我们不希望本书的例子在任何时候崩溃。 即使你不联网，我们也希望你能够开发并运行我们的例子。这就是为什么 Vagrant 开发环境包含一个提供了生成类似Gumtree页面的web服务器。他们可能看起来不像真正的网站好，但从爬虫的角度来看，他们是完全一样的。在你的 Vagrant 开发设备上，你可以在 *http://web:9312/* 点击web服务器，在浏览器打开 *http://localhost:9312/* 。

我们来用scrapy shell 从服务器打开一个页面。
```
$ scrapy shell http://web:9312/properties/property_000000.html
...
[s] Available Scrapy objects:
[s]	crawler	<scrapy.crawler.Crawler object at 0x2d4fb10> [s]	item	{}
[s]	request	<GET http:// web:9312/.../property_000000.html> [s]	response	<200 http://web:9312/.../property_000000.html> [s]	settings	<scrapy.settings.Settings object at 0x2d4fa90> [s]	spider	<DefaultSpider 'default' at 0x3ea0bd0>
[s] Useful shortcuts:

[s]	shelp()	Shell help (print this help)
[s]	fetch(req_or_url) Fetch request (or URL) and update local… [s]	view(response)	View response in a browser
>>>
```
我们得到了一些输出，现在你在一个（Python）环境中，你可以用来测试你刚刚加载的页面（你可以随时退出用Ctrl + D）。

* request and response  (请求和响应)


